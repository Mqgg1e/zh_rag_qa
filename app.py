import streamlit as st
import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# é¡µé¢æ ‡é¢˜
st.title("ğŸ¯ ä¸­æ–‡ RAG è¯„è®ºé—®ç­”æœºå™¨äºº")

# åŠ è½½æ•°æ®
@st.cache_resource
def load_resources():
    df = pd.read_csv("vector_index/faiss_docs.csv")
    corpus = df["clean_text"].tolist()

    index = faiss.read_index("vector_index/faiss_index.index")
    embed_model = SentenceTransformer("shibing624/text2vec-base-multilingual")
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-0.5B-Chat", trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen1.5-0.5B-Chat", trust_remote_code=True).eval()

    return corpus, index, embed_model, tokenizer, model

corpus, index, embed_model, tokenizer, model = load_resources()

# ç”¨æˆ·è¾“å…¥
query = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")

if query:
    with st.spinner("ğŸ” æ£€ç´¢ä¸­..."):
        # å‘é‡åŒ–
        query_vec = embed_model.encode([query]).astype("float32")
        scores, indices = index.search(query_vec, 3)
        context = "\n".join([corpus[i] for i in indices[0]])

        prompt = f"æ ¹æ®ä»¥ä¸‹è¯„è®ºå†…å®¹ï¼Œç®€æ´å›ç­”é—®é¢˜ï¼š{query}\nè¯„è®ºå†…å®¹ï¼š\n{context}"
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)

        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )

        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
        st.markdown("### ğŸ¤– å›ç­”ï¼š")
        st.success(answer)
