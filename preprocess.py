import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from tqdm import tqdm
import os
import re

# 1. åŠ è½½æ¸…æ´—åçš„ä¸­æ–‡è¯„è®ºæ•°æ®
df = pd.read_csv("data/gzxb_cleaned.csv")

# 2. è¿‡æ»¤æ— æ•ˆæ–‡æœ¬ï¼ˆå¤ªçŸ­çš„è¯„è®ºã€ç¼ºå¤±ï¼‰
df = df.dropna(subset=["clean_text"])
df = df[df["clean_text"].str.len() >= 5].drop_duplicates(subset=["clean_text"]).reset_index(drop=True)


def clean_text(text):
    # å»æ‰ä¸­è‹±æ–‡ç¬¦å·æ ‡ç­¾ï¼Œä¾‹å¦‚ã€èµã€‘ã€[èµ]ã€ï¼ˆè¯„è®ºï¼‰ç­‰
    text = re.sub(r"[ã€ã€‘\[\]ï¼ˆï¼‰()ã€Šã€‹<>]", "", text)
    # å»æ‰ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼ã€æ•°å­—æ ‡è®°
    text = re.sub(r"[Â§#ï¿¥@&*~^]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


# df = df[~df["clean_text"].str.contains(r"^([èµå¥½è¯„\d\sã€ã€‘\[\]ï¼ˆï¼‰])+?$")]
df = df[~df["clean_text"].str.contains(r"^(?:[èµå¥½è¯„\d\sã€ã€‘\[\]ï¼ˆï¼‰])+?$")]


df["clean_text"] = df["clean_text"].apply(clean_text)

# 3. åŠ è½½ä¸­æ–‡å‘é‡æ¨¡å‹ï¼ˆCPU ä¹Ÿèƒ½è·‘ï¼Œæ¨èæ¨¡å‹ï¼‰
print("ğŸ”„ æ­£åœ¨åŠ è½½ä¸­æ–‡å‘é‡æ¨¡å‹...")
model = SentenceTransformer("shibing624/text2vec-base-multilingual")

# 4. æ‰¹é‡å°†æ–‡æœ¬è½¬ä¸ºå‘é‡ï¼ˆå‘é‡ç»´åº¦ä¸º 768ï¼‰
print("ğŸ”„ æ­£åœ¨å°†æ–‡æœ¬å‘é‡åŒ–...")
sentences = df["clean_text"].tolist()
embeddings = model.encode(sentences, show_progress_bar=True)

# 5. ä½¿ç”¨ FAISS åˆ›å»ºç´¢å¼•å¹¶ä¿å­˜
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

# åˆ›å»ºä¿å­˜ç›®å½•
os.makedirs("vector_index", exist_ok=True)

# ä¿å­˜ç´¢å¼•æ–‡ä»¶å’ŒåŸå§‹æ–‡æœ¬ç´¢å¼•æ˜ å°„
faiss.write_index(index, "vector_index/faiss_index.index")
df.iloc[:len(sentences)].to_csv("vector_index/faiss_docs.csv", index=False)

print(f"âœ… å‘é‡æ•°é‡: {len(sentences)}ï¼Œç´¢å¼•å·²ä¿å­˜è‡³ vector_index/")
