import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from tqdm import tqdm
import os

# 1. åŠ è½½æ¸…æ´—åçš„ä¸­æ–‡è¯„è®ºæ•°æ®
df = pd.read_csv("data/gzxb_cleaned.csv")

# 2. è¿‡æ»¤æ— æ•ˆæ–‡æœ¬ï¼ˆå¤ªçŸ­çš„è¯„è®ºã€ç¼ºå¤±ï¼‰
df = df.dropna(subset=["clean_text"])
df = df[df["clean_text"].str.len() >= 5].drop_duplicates(subset=["clean_text"]).reset_index(drop=True)

# 3. åŠ è½½ä¸­æ–‡å‘é‡æ¨¡å‹ï¼ˆCPU ä¹Ÿèƒ½è·‘ï¼Œæ¨èæ¨¡å‹ï¼‰
print("ğŸ”„ æ­£åœ¨åŠ è½½ä¸­æ–‡å‘é‡æ¨¡å‹...")
model = SentenceTransformer("shibing624/text2vec-base-multilingual")

# 4. æ‰¹é‡å°†æ–‡æœ¬è½¬ä¸ºå‘é‡ï¼ˆå‘é‡ç»´åº¦ä¸º 768ï¼‰
print("ğŸ”„ æ­£åœ¨å°†æ–‡æœ¬å‘é‡åŒ–...")
sentences = df["clean_text"].tolist()
embeddings = model.encode(sentences, show_progress_bar=True)

# 5. ä½¿ç”¨ FAISS åˆ›å»ºç´¢å¼•å¹¶ä¿å­˜
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

# åˆ›å»ºä¿å­˜ç›®å½•
os.makedirs("vector_index", exist_ok=True)

# ä¿å­˜ç´¢å¼•æ–‡ä»¶å’ŒåŸå§‹æ–‡æœ¬ç´¢å¼•æ˜ å°„
faiss.write_index(index, "vector_index/faiss_index.index")
df.iloc[:len(sentences)].to_csv("vector_index/faiss_docs.csv", index=False)

print(f"âœ… å‘é‡æ•°é‡: {len(sentences)}ï¼Œç´¢å¼•å·²ä¿å­˜è‡³ vector_index/")

