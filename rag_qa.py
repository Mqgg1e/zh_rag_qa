# rag_qa.py

import faiss
import pandas as pd
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os

# è®¾ç½®è·¯å¾„
INDEX_PATH = "vector_index/faiss_index.index"
CSV_PATH = "vector_index/faiss_docs.csv"

# åŠ è½½è¯„è®ºæ–‡æœ¬
df = pd.read_csv(CSV_PATH)
corpus = df["comment"].tolist()

# åŠ è½½å‘é‡ç´¢å¼•
index = faiss.read_index(INDEX_PATH)

# åŠ è½½ embedding æ¨¡å‹ï¼ˆå’Œ preprocess.py ä¸­ä¸€è‡´ï¼‰
embed_model = SentenceTransformer("shibing624/text2vec-base-multilingual")

# åŠ è½½æœ¬åœ°æˆ– API çš„ LLM â€”â€” è¿™é‡Œå‡è®¾ä½ ä½¿ç”¨ HuggingFace æœ¬åœ°æ¨¡å‹ï¼ˆå¯æ›¿æ¢ä¸º OpenAI/LLaMAï¼‰
# ä½ å¯ä»¥æ›¿æ¢ä¸º openai.ChatCompletion API
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-0.5B-Chat", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen1.5-0.5B-Chat", trust_remote_code=True)

# è·å–ç”¨æˆ·è¾“å…¥
query = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")

query_vec = embed_model.encode([query])
query_vec = query_vec.astype("float32")

top_k = 3
scores, indices = index.search(query_vec, top_k)

retrieved = [corpus[i] for i in indices[0]]
context = "\n".join(retrieved)

prompt = f"æ ¹æ®ä»¥ä¸‹è¯„è®ºå†…å®¹ï¼Œç®€æ´å›ç­”é—®é¢˜ï¼š{query}\nè¯„è®ºå†…å®¹ï¼š\n{context}"


inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)

outputs = model.generate(
    **inputs,
    max_new_tokens=150,
    no_repeat_ngram_size=3,
    early_stopping=False,  # å…³é—­ï¼Œæˆ–è€…è®¾ç½® num_beams>1 æ‰ç”¨
    do_sample=True,        # é‡‡æ ·æ¨¡å¼å¼€å¯ temperature æ‰ç”Ÿæ•ˆ
    temperature=0.7,
    top_p=0.9,             # ä¹Ÿå¯ä»¥ç”¨ nucleus sampling é™åˆ¶é‡‡æ ·èŒƒå›´ï¼Œé˜²æ­¢æ— æ„ä¹‰è¯
    top_k=50               # é™åˆ¶é‡‡æ ·æ—¶åªä»æ¦‚ç‡æœ€é«˜çš„50ä¸ªtokené‡‡æ ·
)

# decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
# print("æ¨¡å‹åŸå§‹è¾“å‡º:", decoded)
tokens = outputs[0].tolist()
text = "".join([tokenizer.decode([t]) for t in tokens if t not in tokenizer.all_special_ids])
print("æ¨¡å‹è¾“å‡ºæ‹¼æ¥å:", text)

print("\nğŸ§  å›ç­”ï¼š")

answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
print([tokenizer.decode([token]) for token in outputs[0][:20]])
print("\nğŸ§  å›ç­”ï¼š")
print(answer)
