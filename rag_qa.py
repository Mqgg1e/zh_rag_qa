# rag_qa.py

import faiss
import pandas as pd
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os

# è®¾ç½®è·¯å¾„
INDEX_PATH = "vector_index/faiss.index"
CSV_PATH = "vector_index/gzxb_cleaned.csv"

# åŠ è½½è¯„è®ºæ–‡æœ¬
df = pd.read_csv(CSV_PATH)
corpus = df["comment"].tolist()

# åŠ è½½å‘é‡ç´¢å¼•
index = faiss.read_index(INDEX_PATH)

# åŠ è½½ embedding æ¨¡å‹ï¼ˆå’Œ preprocess.py ä¸­ä¸€è‡´ï¼‰
embed_model = SentenceTransformer("shibing624/text2vec-base-chinese")

# åŠ è½½æœ¬åœ°æˆ– API çš„ LLM â€”â€” è¿™é‡Œå‡è®¾ä½ ä½¿ç”¨ HuggingFace æœ¬åœ°æ¨¡å‹ï¼ˆå¯æ›¿æ¢ä¸º OpenAI/LLaMAï¼‰
# ä½ å¯ä»¥æ›¿æ¢ä¸º openai.ChatCompletion API
tokenizer = AutoTokenizer.from_pretrained("IDEA-CCNL/Randeng-Pegasus-238M-Chinese", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("IDEA-CCNL/Randeng-Pegasus-238M-Chinese", trust_remote_code=True)

# è·å–ç”¨æˆ·è¾“å…¥
query = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")

# ç¼–ç é—®é¢˜ä¸ºå‘é‡
query_vec = embed_model.encode([query])
query_vec = query_vec.astype("float32")

# æ£€ç´¢ç›¸ä¼¼è¯„è®º
top_k = 3
scores, indices = index.search(query_vec, top_k)

# å–å‡ºæœ€ç›¸å…³çš„æ–‡æœ¬
retrieved = [corpus[i] for i in indices[0]]
context = "\n".join(retrieved)

# æ‹¼æ¥æç¤ºè¯
prompt = f"ä»¥ä¸‹æ˜¯ä¸€äº›ä¸é—®é¢˜ç›¸å…³çš„è¯„è®ºï¼š\n{context}\n\nè¯·æ ¹æ®è¿™äº›å†…å®¹å›ç­”é—®é¢˜ï¼š{query}"

# æ„é€ è¾“å…¥
inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
outputs = model.generate(**inputs, max_new_tokens=150)
answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("\nğŸ§  å›ç­”ï¼š")
print(answer)
